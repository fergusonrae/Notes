{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Intelligence: A Modern Approach, 3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Agents and Environments\n",
    "\n",
    "An agent is anything that can be viewed as percieving its environment through sensors and acting upon that environment through actuators. Example, human agents have eyes, ears and other organs for sensors and hands, legs, vocal chords, etc for actuators.\n",
    "\n",
    "A software agent recieve keystrokes, file contents, and packets for sensory input and displays the screen, writes files, and sends packets for actuators.\n",
    "\n",
    "Percept = the agent's perceptual inputs at any given instant.\n",
    "\n",
    "Percept sequence = the complete history of everything the agent has ever percieved.\n",
    "\n",
    "In general, an agent's choice of action at any given instant can depend on the entire percept sequence observed to date, but not on anything it hasn't percieved.\n",
    "\n",
    "In math speak, an agent's behavior is described by the **agent function** that maps any given percept sequence to an action. This agent function will be implemented by an **agent program**. These are two distinct ideas and should be kept seperately. The agent fuction is an abstract mathmatical description, the agent program is the concrete implementation, running within some physical system.\n",
    "\n",
    "The world is not divided into agents and non-agents. Rather, an agent is a tool for analyzing systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Good Behavior: The Concept of Rationality\n",
    "\n",
    "A **rational agent** is one that does the right things. Or maximizes its performance measure. The right thing is determined by a performance measure from the environment. An agent should not be able to decide for itself if it was rational.\n",
    "\n",
    "As a general rule: design performance measures to reflect what you want in the environment, not what you want the agent to behave as. You also need to think of all the ways the agent could meet the performance measure to control for ways it could meet it in a bad way.\n",
    "\n",
    "## Rationality\n",
    "\n",
    "What is rational at any given time depends on four things:\n",
    "- The performance measure that defines the criterion of success\n",
    "- An agent's prior knowledge of the environment\n",
    "- The actions that the agent can perform\n",
    "- The agent's percept sequence to date\n",
    "\n",
    "## Omniscience, Learning, and Autonomy\n",
    "\n",
    "Omniscient agent knows the actual outcome and can act accordingly, but is impossible in reality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
