{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is it?\n",
    "\n",
    "A programming model and an implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster.\n",
    "\n",
    "It is composed of a Map() procedure that does filtering and sorting along with a Reduce() method that does a summary opertation. So, kind of like a grouby() in Python.\n",
    "\n",
    "It is a specialization of the \"split-apply-combine\" strategy for data analysis. And any gains in speed or efficiency will only be seen if there are multiple threads, otherwise it is just as fast. Because if there are not multiple threads, it's not being used in parallel.\n",
    "\n",
    "In this, computers are called __nodes__ and a collection of nodes is called a __cluster__ if all nodes are on the same local network and use a similiar hardware or a __grid__ if they are shared across geographically and administratively distributed systems.\n",
    "\n",
    "# Steps\n",
    "\n",
    "* Map Step:\n",
    " * Each worker node applies the \"map()\" function to the local data, and writes the output to a temporary storage. A master node ensures that only one copy of redundant input data is processed.\n",
    "* Shuffle Step:\n",
    " * Worker nodes redistribute data based on the output keys (produced by the Map() function) such that all data belonging to one key is located on the same worker node.\n",
    "* Reduce Step\n",
    " * Worker nodes now process each group of output data, per key, in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
